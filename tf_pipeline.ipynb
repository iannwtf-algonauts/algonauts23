{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-13 13:52:45.799327: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import pearsonr as corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context manager to suppress stdout\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose where to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform = 'jupyter_notebook' #@param ['colab', 'jupyter_notebook'] {allow-input: true}\n",
    "\n",
    "if platform == 'colab':\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/', force_remount=True)\n",
    "    data_dir = '/content/drive/MyDrive/algonauts_2023_tutorial_data' #@param {type:\"string\"}\n",
    "    parent_submission_dir = '/content/drive/MyDrive/algonauts_2023_challenge_submission' #@param {type:\"string\"}\n",
    "\n",
    "if platform == 'jupyter_notebook':\n",
    "    data_dir = './algonauts_2023_challenge_data'\n",
    "    parent_submission_dir = './algonauts_2023_challenge_submission'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_correlation(lh_correlation, rh_correlation, data_dir):\n",
    "    # Load the ROI classes mapping dictionaries\n",
    "    roi_mapping_files = ['mapping_prf-visualrois.npy', 'mapping_floc-bodies.npy',\n",
    "        'mapping_floc-faces.npy', 'mapping_floc-places.npy',\n",
    "        'mapping_floc-words.npy', 'mapping_streams.npy']\n",
    "    roi_name_maps = []\n",
    "    for r in roi_mapping_files:\n",
    "        roi_name_maps.append(np.load(os.path.join(data_dir, 'roi_masks', r),\n",
    "            allow_pickle=True).item())\n",
    "\n",
    "    # Load the ROI brain surface maps\n",
    "    lh_challenge_roi_files = ['lh.prf-visualrois_challenge_space.npy',\n",
    "        'lh.floc-bodies_challenge_space.npy', 'lh.floc-faces_challenge_space.npy',\n",
    "        'lh.floc-places_challenge_space.npy', 'lh.floc-words_challenge_space.npy',\n",
    "        'lh.streams_challenge_space.npy']\n",
    "    rh_challenge_roi_files = ['rh.prf-visualrois_challenge_space.npy',\n",
    "        'rh.floc-bodies_challenge_space.npy', 'rh.floc-faces_challenge_space.npy',\n",
    "        'rh.floc-places_challenge_space.npy', 'rh.floc-words_challenge_space.npy',\n",
    "        'rh.streams_challenge_space.npy']\n",
    "    lh_challenge_rois = []\n",
    "    rh_challenge_rois = []\n",
    "    for r in range(len(lh_challenge_roi_files)):\n",
    "        lh_challenge_rois.append(np.load(os.path.join(data_dir, 'roi_masks',\n",
    "            lh_challenge_roi_files[r])))\n",
    "        rh_challenge_rois.append(np.load(os.path.join(data_dir, 'roi_masks',\n",
    "            rh_challenge_roi_files[r])))\n",
    "\n",
    "    # Select the correlation results vertices of each ROI\n",
    "    roi_names = []\n",
    "    lh_roi_correlation = []\n",
    "    rh_roi_correlation = []\n",
    "    for r1 in range(len(lh_challenge_rois)):\n",
    "        for r2 in roi_name_maps[r1].items():\n",
    "            if r2[0] != 0: # zeros indicate to vertices falling outside the ROI of interest\n",
    "                roi_names.append(r2[1])\n",
    "                lh_roi_idx = np.where(lh_challenge_rois[r1] == r2[0])[0]\n",
    "                rh_roi_idx = np.where(rh_challenge_rois[r1] == r2[0])[0]\n",
    "                lh_roi_correlation.append(lh_correlation[lh_roi_idx])\n",
    "                rh_roi_correlation.append(rh_correlation[rh_roi_idx])\n",
    "    roi_names.append('All vertices')\n",
    "    lh_roi_correlation.append(lh_correlation)\n",
    "    rh_roi_correlation.append(rh_correlation)\n",
    "\n",
    "    # Create the plot\n",
    "    lh_median_roi_correlation = [np.median(lh_roi_correlation[r])\n",
    "        for r in range(len(lh_roi_correlation))]\n",
    "    rh_median_roi_correlation = [np.median(rh_roi_correlation[r])\n",
    "        for r in range(len(rh_roi_correlation))]\n",
    "    plt.figure(figsize=(18,6))\n",
    "    x = np.arange(len(roi_names))\n",
    "    width = 0.30\n",
    "    plt.bar(x - width/2, lh_median_roi_correlation, width, label='Left Hemisphere')\n",
    "    plt.bar(x + width/2, rh_median_roi_correlation, width,\n",
    "        label='Right Hemishpere')\n",
    "    plt.xlim(left=min(x)-.5, right=max(x)+.5)\n",
    "    plt.ylim(bottom=0, top=1)\n",
    "    plt.xlabel('ROIs')\n",
    "    plt.xticks(ticks=x, labels=roi_names, rotation=60)\n",
    "    plt.ylabel('Median Pearson\\'s $r$')\n",
    "    plt.legend(frameon=True, loc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calulate_correlaction(lh_fmri_val, lh_fmri_val_pred, rh_fmri_val, rh_fmri_val_pred):\n",
    "    # Empty correlation array of shape: (LH vertices)\n",
    "    lh_correlation = np.zeros(lh_fmri_val_pred.shape[1])\n",
    "    # Correlate each predicted LH vertex with the corresponding ground truth vertex\n",
    "    for v in range(lh_fmri_val_pred.shape[1]):\n",
    "        lh_correlation[v] = corr(lh_fmri_val_pred[:,v], lh_fmri_val[:,v])[0]\n",
    "\n",
    "    # Empty correlation array of shape: (RH vertices)\n",
    "    rh_correlation = np.zeros(rh_fmri_val_pred.shape[1])\n",
    "    # Correlate each predicted RH vertex with the corresponding ground truth vertex\n",
    "    for v in range(rh_fmri_val_pred.shape[1]):\n",
    "        rh_correlation[v] = corr(rh_fmri_val_pred[:,v], rh_fmri_val[:,v])[0]\n",
    "    return lh_correlation, rh_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(lh_fmri_test_pred, rh_fmri_test_pred, subject_submission_dir):\n",
    "    lh_fmri_test_pred = lh_fmri_test_pred.astype(np.float32)\n",
    "    rh_fmri_test_pred = rh_fmri_test_pred.astype(np.float32)\n",
    "\n",
    "    np.save(os.path.join(subject_submission_dir, 'lh_pred_test.npy'), lh_fmri_test_pred)\n",
    "    np.save(os.path.join(subject_submission_dir, 'rh_pred_test.npy'), rh_fmri_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fmri data and image datasets per subject\n",
    "def load_FMRI_and_images(subj_dir, create_dataset_from_image_paths):\n",
    "    train_img_dir  = os.path.join(subj_dir, 'training_split', 'training_images')\n",
    "    test_img_dir  = os.path.join(subj_dir, 'test_split', 'test_images')\n",
    "    train_img_list = os.listdir(train_img_dir)\n",
    "    # Create a dataset\n",
    "    batch_size = 300\n",
    "    split = 0.9\n",
    "    num_batches_train = int(len(train_img_list) * split // batch_size)\n",
    "    num_samples_train = int(num_batches_train * batch_size)\n",
    "    # Handle frmi files\n",
    "    fmri_dir = os.path.join(subj_dir, 'training_split', 'training_fmri')\n",
    "    lh_fmri = np.load(os.path.join(fmri_dir, 'lh_training_fmri.npy'))\n",
    "    rh_fmri = np.load(os.path.join(fmri_dir, 'rh_training_fmri.npy'))\n",
    "\n",
    "    print('LH training fMRI data shape:')\n",
    "    print(lh_fmri.shape)\n",
    "    print('(Training stimulus images × LH vertices)')\n",
    "\n",
    "    print('\\nRH training fMRI data shape:')\n",
    "    print(rh_fmri.shape)\n",
    "    print('(Training stimulus images × RH vertices)')\n",
    "    lh_fmri_train = lh_fmri[:num_samples_train]\n",
    "    lh_fmri_val = lh_fmri[num_samples_train:]\n",
    "    rh_fmri_train = rh_fmri[:num_samples_train]\n",
    "    rh_fmri_val = rh_fmri[num_samples_train:]\n",
    "\n",
    "    dataset = create_dataset_from_image_paths(train_img_dir, batch_size)\n",
    "    # create train and validation dataset from the training dataset (90% train, 10% validation)\n",
    "    train_dataset = dataset.take(num_batches_train)\n",
    "    val_dataset = dataset.skip(num_batches_train)\n",
    "\n",
    "    test_dataset = create_dataset_from_image_paths(test_img_dir, batch_size)\n",
    "\n",
    "    print('Training dataset size: ' + str(len(train_dataset)))\n",
    "    print('Validation dataset size: ' + str(len(val_dataset)))\n",
    "    print('Test dataset size: ' + str(len(test_dataset)) + '\\n')\n",
    "\n",
    "    return (train_dataset, val_dataset, test_dataset, lh_fmri_train, lh_fmri_val, rh_fmri_train, rh_fmri_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(subj_dir, readout_layer, base_model, lh_fmri_train, rh_fmri_train, lh_fmri_val, rh_fmri_val, train_dataset, val_dataset, test_dataset):\n",
    "    \n",
    "    model = tf.keras.Model(inputs=base_model.input, outputs=base_model.get_layer(readout_layer).output)\n",
    "    pca = PCA(n_components=100)\n",
    "\n",
    "    # Fit PCA to batch of datastet\n",
    "    for batch in train_dataset:\n",
    "        with HiddenPrints():\n",
    "            # Extract features\n",
    "            features = model.predict(batch)\n",
    "            # Flatten the features\n",
    "            features = features.reshape(features.shape[0], -1)\n",
    "            # Fit PCA to batch of features\n",
    "            pca.fit(features)\n",
    "\n",
    "    # print('plot the explained variance')\n",
    "    # plt.plot(pca.explained_variance_ratio_)\n",
    "    # plt.show()\n",
    "    # print('plot the cumulative explained variance')\n",
    "    # plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    # plt.show()\n",
    "\n",
    "    def extract_features(dataset, model, pca):\n",
    "        features = []\n",
    "        for batch in dataset:\n",
    "            with HiddenPrints():\n",
    "                ft = model.predict(batch)\n",
    "                # Flatten the features\n",
    "                ft = ft.reshape(ft.shape[0], -1)\n",
    "                # Fit PCA to batch of features\n",
    "                ft = pca.transform(ft)\n",
    "            features.append(ft)\n",
    "        return np.vstack(features)\n",
    "\n",
    "    features_train = extract_features(train_dataset, model, pca)\n",
    "    features_val = extract_features(val_dataset, model, pca)\n",
    "    features_test = extract_features(test_dataset, model, pca)\n",
    "\n",
    "    del model, pca\n",
    "    # Fitting regression\n",
    "    reg_lh = LinearRegression().fit(features_train, lh_fmri_train)\n",
    "    reg_rh = LinearRegression().fit(features_train, rh_fmri_train)\n",
    "    # Use fitted linear regressions to predict the validation and test fMRI data\n",
    "    lh_fmri_val_pred = reg_lh.predict(features_val)\n",
    "    lh_fmri_test_pred = reg_lh.predict(features_test)\n",
    "    rh_fmri_val_pred = reg_rh.predict(features_val)\n",
    "    rh_fmri_test_pred = reg_rh.predict(features_test)\n",
    "    # Calculate correlation\n",
    "    lh_correlation, rh_correlation = calulate_correlaction(lh_fmri_val, lh_fmri_val_pred, rh_fmri_val, rh_fmri_val_pred)\n",
    "    print('Left hemisphere median correlation: ', np.median(lh_correlation))\n",
    "    print('Right hemisphere median correlation: ', np.median(rh_correlation))\n",
    "    # Visualize\n",
    "    visualize_correlation(lh_correlation, rh_correlation, subj_dir)\n",
    "    return (lh_fmri_test_pred, rh_fmri_test_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify image preprocessing steps, model and layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_3 -> conv1_pad -> conv1_conv -> conv1_bn -> conv1_relu -> pool1_pad -> pool1_pool -> conv2_block1_1_conv -> conv2_block1_1_bn -> conv2_block1_1_relu -> conv2_block1_2_conv -> conv2_block1_2_bn -> conv2_block1_2_relu -> conv2_block1_0_conv -> conv2_block1_3_conv -> conv2_block1_0_bn -> conv2_block1_3_bn -> conv2_block1_add -> conv2_block1_out -> conv2_block2_1_conv -> conv2_block2_1_bn -> conv2_block2_1_relu -> conv2_block2_2_conv -> conv2_block2_2_bn -> conv2_block2_2_relu -> conv2_block2_3_conv -> conv2_block2_3_bn -> conv2_block2_add -> conv2_block2_out -> conv2_block3_1_conv -> conv2_block3_1_bn -> conv2_block3_1_relu -> conv2_block3_2_conv -> conv2_block3_2_bn -> conv2_block3_2_relu -> conv2_block3_3_conv -> conv2_block3_3_bn -> conv2_block3_add -> conv2_block3_out -> conv3_block1_1_conv -> conv3_block1_1_bn -> conv3_block1_1_relu -> conv3_block1_2_conv -> conv3_block1_2_bn -> conv3_block1_2_relu -> conv3_block1_0_conv -> conv3_block1_3_conv -> conv3_block1_0_bn -> conv3_block1_3_bn -> conv3_block1_add -> conv3_block1_out -> conv3_block2_1_conv -> conv3_block2_1_bn -> conv3_block2_1_relu -> conv3_block2_2_conv -> conv3_block2_2_bn -> conv3_block2_2_relu -> conv3_block2_3_conv -> conv3_block2_3_bn -> conv3_block2_add -> conv3_block2_out -> conv3_block3_1_conv -> conv3_block3_1_bn -> conv3_block3_1_relu -> conv3_block3_2_conv -> conv3_block3_2_bn -> conv3_block3_2_relu -> conv3_block3_3_conv -> conv3_block3_3_bn -> conv3_block3_add -> conv3_block3_out -> conv3_block4_1_conv -> conv3_block4_1_bn -> conv3_block4_1_relu -> conv3_block4_2_conv -> conv3_block4_2_bn -> conv3_block4_2_relu -> conv3_block4_3_conv -> conv3_block4_3_bn -> conv3_block4_add -> conv3_block4_out -> conv4_block1_1_conv -> conv4_block1_1_bn -> conv4_block1_1_relu -> conv4_block1_2_conv -> conv4_block1_2_bn -> conv4_block1_2_relu -> conv4_block1_0_conv -> conv4_block1_3_conv -> conv4_block1_0_bn -> conv4_block1_3_bn -> conv4_block1_add -> conv4_block1_out -> conv4_block2_1_conv -> conv4_block2_1_bn -> conv4_block2_1_relu -> conv4_block2_2_conv -> conv4_block2_2_bn -> conv4_block2_2_relu -> conv4_block2_3_conv -> conv4_block2_3_bn -> conv4_block2_add -> conv4_block2_out -> conv4_block3_1_conv -> conv4_block3_1_bn -> conv4_block3_1_relu -> conv4_block3_2_conv -> conv4_block3_2_bn -> conv4_block3_2_relu -> conv4_block3_3_conv -> conv4_block3_3_bn -> conv4_block3_add -> conv4_block3_out -> conv4_block4_1_conv -> conv4_block4_1_bn -> conv4_block4_1_relu -> conv4_block4_2_conv -> conv4_block4_2_bn -> conv4_block4_2_relu -> conv4_block4_3_conv -> conv4_block4_3_bn -> conv4_block4_add -> conv4_block4_out -> conv4_block5_1_conv -> conv4_block5_1_bn -> conv4_block5_1_relu -> conv4_block5_2_conv -> conv4_block5_2_bn -> conv4_block5_2_relu -> conv4_block5_3_conv -> conv4_block5_3_bn -> conv4_block5_add -> conv4_block5_out -> conv4_block6_1_conv -> conv4_block6_1_bn -> conv4_block6_1_relu -> conv4_block6_2_conv -> conv4_block6_2_bn -> conv4_block6_2_relu -> conv4_block6_3_conv -> conv4_block6_3_bn -> conv4_block6_add -> conv4_block6_out -> conv5_block1_1_conv -> conv5_block1_1_bn -> conv5_block1_1_relu -> conv5_block1_2_conv -> conv5_block1_2_bn -> conv5_block1_2_relu -> conv5_block1_0_conv -> conv5_block1_3_conv -> conv5_block1_0_bn -> conv5_block1_3_bn -> conv5_block1_add -> conv5_block1_out -> conv5_block2_1_conv -> conv5_block2_1_bn -> conv5_block2_1_relu -> conv5_block2_2_conv -> conv5_block2_2_bn -> conv5_block2_2_relu -> conv5_block2_3_conv -> conv5_block2_3_bn -> conv5_block2_add -> conv5_block2_out -> conv5_block3_1_conv -> conv5_block3_1_bn -> conv5_block3_1_relu -> conv5_block3_2_conv -> conv5_block3_2_bn -> conv5_block3_2_relu -> conv5_block3_3_conv -> conv5_block3_3_bn -> conv5_block3_add -> conv5_block3_out -> avg_pool -> predictions -> "
     ]
    }
   ],
   "source": [
    "def create_dataset_from_image_paths(image_dir, batch_size):\n",
    "    dataset = tf.keras.utils.image_dataset_from_directory(image_dir, image_size=(224, 224), labels=None, shuffle=False, batch_size=batch_size)\n",
    "    dataset = dataset.map(tf.keras.applications.efficientnet.preprocess_input)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Take models from here https://keras.io/api/applications/\n",
    "base_model = tf.keras.applications.ResNet50(weights='imagenet')\n",
    "for layer in base_model.layers:\n",
    "    print(layer.name, end=' -> ')\n",
    "\n",
    "layers = ['avg_pool'] #, 'block5c_bn', 'block5b_bn', 'block5a_bn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LH training fMRI data shape:\n",
      "(9841, 19004)\n",
      "(Training stimulus images × LH vertices)\n",
      "\n",
      "RH training fMRI data shape:\n",
      "(9841, 20544)\n",
      "(Training stimulus images × RH vertices)\n",
      "Found 9841 files belonging to 1 classes.\n",
      "Found 159 files belonging to 1 classes.\n",
      "Training dataset size: 29\n",
      "Validation dataset size: 4\n",
      "Test dataset size: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subj = 'subj01'\n",
    "subj_dir = os.path.join(data_dir, subj)\n",
    "train_dataset, val_dataset, test_dataset, lh_fmri_train, lh_fmri_val, rh_fmri_train, rh_fmri_val = load_FMRI_and_images(subj_dir, create_dataset_from_image_paths)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readout layer: avg_pool\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for readout_layer in layers:\n",
    "    print(f'Readout layer: {readout_layer}\\n')\n",
    "    lh_fmri_test_pred, rh_fmri_test_pred = run_pipeline(subj_dir, readout_layer, base_model, lh_fmri_train, rh_fmri_train, lh_fmri_val, rh_fmri_val, train_dataset, val_dataset, test_dataset)\n",
    "    save_predictions(lh_fmri_test_pred, rh_fmri_test_pred, os.path.join(parent_submission_dir, subj_dir))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "365c028d1c840c93b0f470009ef966704a741407ca08d29f5bd0c0c6373757cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

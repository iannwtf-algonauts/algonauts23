{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose where to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform = 'jupyter_notebook' #@param ['colab', 'jupyter_notebook'] {allow-input: true}\n",
    "\n",
    "if platform == 'colab':\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/', force_remount=True)\n",
    "    data_dir = '/content/drive/MyDrive/algonauts_2023_tutorial_data' #@param {type:\"string\"}\n",
    "    parent_submission_dir = '/content/drive/MyDrive/algonauts_2023_challenge_submission' #@param {type:\"string\"}\n",
    "\n",
    "if platform == 'jupyter_notebook':\n",
    "    data_dir = './algonauts_2023_challenge_data'\n",
    "    parent_submission_dir = './algonauts_2023_challenge_submission'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import pearsonr as corr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_dir = 'subj01'\n",
    "data_dir = os.path.join(data_dir, subj_dir)\n",
    "train_img_dir  = os.path.join(data_dir, 'training_split', 'training_images')\n",
    "test_img_dir  = os.path.join(data_dir, 'test_split', 'test_images')\n",
    "train_img_list = os.listdir(train_img_dir)\n",
    "test_img_list = os.listdir(test_img_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 8857\n",
      "Validation images: 984\n",
      "Test images: 159\n"
     ]
    }
   ],
   "source": [
    "train_img_list.sort()\n",
    "test_img_list.sort()\n",
    "\n",
    "rand_seed = 5\n",
    "np.random.seed(rand_seed)\n",
    "num_train = int(np.round(len(train_img_list) / 100 * 90))\n",
    "idxs = np.arange(len(train_img_list))\n",
    "np.random.shuffle(idxs)\n",
    "idxs_train, idxs_val = idxs[:num_train], idxs[num_train:]\n",
    "idxs_test = np.arange(len(test_img_list))\n",
    "\n",
    "train_img_paths = [os.path.join(train_img_dir, train_img_file) for train_img_file in train_img_list[:num_train]]\n",
    "val_img_paths = [os.path.join(train_img_dir, train_img_file) for train_img_file in train_img_list[num_train:]]\n",
    "test_img_paths = [os.path.join(test_img_dir, test_img_file) for test_img_file in test_img_list]\n",
    "print('Training images: ' + str(len(train_img_paths)))\n",
    "print('Validation images: ' + str(len(val_img_paths)))\n",
    "print('Test images: ' + str(len(test_img_list)))\n",
    "\n",
    "# SHORTING THE TRAINING IMAGES DATASET FOR TESTING\n",
    "# # train_img_paths = train_img_paths[:300]\n",
    "# val_img_paths = val_img_paths[:300]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing images specific to ResNet50\n",
    "def load_and_preprocess_images(images):\n",
    "    images = [tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224)) for image_path in images] # they do the same resizing in the tutorial\n",
    "    images = [tf.keras.preprocessing.image.img_to_array(image) for image in images] # 224*224*3\n",
    "    images = [tf.keras.applications.resnet50.preprocess_input(image) for image in images] # 224*224*3 (preprocessed - what does it do exactly?)\n",
    "    return images\n",
    "\n",
    "train_images = load_and_preprocess_images(train_img_paths)\n",
    "val_images = load_and_preprocess_images(val_img_paths)\n",
    "test_images = load_and_preprocess_images(test_img_paths)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a model and a readout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 21:05:39.442938: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_1\n",
      "conv1_pad\n",
      "conv1_conv\n",
      "conv1_bn\n",
      "conv1_relu\n",
      "pool1_pad\n",
      "pool1_pool\n",
      "conv2_block1_1_conv\n",
      "conv2_block1_1_bn\n",
      "conv2_block1_1_relu\n",
      "conv2_block1_2_conv\n",
      "conv2_block1_2_bn\n",
      "conv2_block1_2_relu\n",
      "conv2_block1_0_conv\n",
      "conv2_block1_3_conv\n",
      "conv2_block1_0_bn\n",
      "conv2_block1_3_bn\n",
      "conv2_block1_add\n",
      "conv2_block1_out\n",
      "conv2_block2_1_conv\n",
      "conv2_block2_1_bn\n",
      "conv2_block2_1_relu\n",
      "conv2_block2_2_conv\n",
      "conv2_block2_2_bn\n",
      "conv2_block2_2_relu\n",
      "conv2_block2_3_conv\n",
      "conv2_block2_3_bn\n",
      "conv2_block2_add\n",
      "conv2_block2_out\n",
      "conv2_block3_1_conv\n",
      "conv2_block3_1_bn\n",
      "conv2_block3_1_relu\n",
      "conv2_block3_2_conv\n",
      "conv2_block3_2_bn\n",
      "conv2_block3_2_relu\n",
      "conv2_block3_3_conv\n",
      "conv2_block3_3_bn\n",
      "conv2_block3_add\n",
      "conv2_block3_out\n",
      "conv3_block1_1_conv\n",
      "conv3_block1_1_bn\n",
      "conv3_block1_1_relu\n",
      "conv3_block1_2_conv\n",
      "conv3_block1_2_bn\n",
      "conv3_block1_2_relu\n",
      "conv3_block1_0_conv\n",
      "conv3_block1_3_conv\n",
      "conv3_block1_0_bn\n",
      "conv3_block1_3_bn\n",
      "conv3_block1_add\n",
      "conv3_block1_out\n",
      "conv3_block2_1_conv\n",
      "conv3_block2_1_bn\n",
      "conv3_block2_1_relu\n",
      "conv3_block2_2_conv\n",
      "conv3_block2_2_bn\n",
      "conv3_block2_2_relu\n",
      "conv3_block2_3_conv\n",
      "conv3_block2_3_bn\n",
      "conv3_block2_add\n",
      "conv3_block2_out\n",
      "conv3_block3_1_conv\n",
      "conv3_block3_1_bn\n",
      "conv3_block3_1_relu\n",
      "conv3_block3_2_conv\n",
      "conv3_block3_2_bn\n",
      "conv3_block3_2_relu\n",
      "conv3_block3_3_conv\n",
      "conv3_block3_3_bn\n",
      "conv3_block3_add\n",
      "conv3_block3_out\n",
      "conv3_block4_1_conv\n",
      "conv3_block4_1_bn\n",
      "conv3_block4_1_relu\n",
      "conv3_block4_2_conv\n",
      "conv3_block4_2_bn\n",
      "conv3_block4_2_relu\n",
      "conv3_block4_3_conv\n",
      "conv3_block4_3_bn\n",
      "conv3_block4_add\n",
      "conv3_block4_out\n",
      "conv4_block1_1_conv\n",
      "conv4_block1_1_bn\n",
      "conv4_block1_1_relu\n",
      "conv4_block1_2_conv\n",
      "conv4_block1_2_bn\n",
      "conv4_block1_2_relu\n",
      "conv4_block1_0_conv\n",
      "conv4_block1_3_conv\n",
      "conv4_block1_0_bn\n",
      "conv4_block1_3_bn\n",
      "conv4_block1_add\n",
      "conv4_block1_out\n",
      "conv4_block2_1_conv\n",
      "conv4_block2_1_bn\n",
      "conv4_block2_1_relu\n",
      "conv4_block2_2_conv\n",
      "conv4_block2_2_bn\n",
      "conv4_block2_2_relu\n",
      "conv4_block2_3_conv\n",
      "conv4_block2_3_bn\n",
      "conv4_block2_add\n",
      "conv4_block2_out\n",
      "conv4_block3_1_conv\n",
      "conv4_block3_1_bn\n",
      "conv4_block3_1_relu\n",
      "conv4_block3_2_conv\n",
      "conv4_block3_2_bn\n",
      "conv4_block3_2_relu\n",
      "conv4_block3_3_conv\n",
      "conv4_block3_3_bn\n",
      "conv4_block3_add\n",
      "conv4_block3_out\n",
      "conv4_block4_1_conv\n",
      "conv4_block4_1_bn\n",
      "conv4_block4_1_relu\n",
      "conv4_block4_2_conv\n",
      "conv4_block4_2_bn\n",
      "conv4_block4_2_relu\n",
      "conv4_block4_3_conv\n",
      "conv4_block4_3_bn\n",
      "conv4_block4_add\n",
      "conv4_block4_out\n",
      "conv4_block5_1_conv\n",
      "conv4_block5_1_bn\n",
      "conv4_block5_1_relu\n",
      "conv4_block5_2_conv\n",
      "conv4_block5_2_bn\n",
      "conv4_block5_2_relu\n",
      "conv4_block5_3_conv\n",
      "conv4_block5_3_bn\n",
      "conv4_block5_add\n",
      "conv4_block5_out\n",
      "conv4_block6_1_conv\n",
      "conv4_block6_1_bn\n",
      "conv4_block6_1_relu\n",
      "conv4_block6_2_conv\n",
      "conv4_block6_2_bn\n",
      "conv4_block6_2_relu\n",
      "conv4_block6_3_conv\n",
      "conv4_block6_3_bn\n",
      "conv4_block6_add\n",
      "conv4_block6_out\n",
      "conv5_block1_1_conv\n",
      "conv5_block1_1_bn\n",
      "conv5_block1_1_relu\n",
      "conv5_block1_2_conv\n",
      "conv5_block1_2_bn\n",
      "conv5_block1_2_relu\n",
      "conv5_block1_0_conv\n",
      "conv5_block1_3_conv\n",
      "conv5_block1_0_bn\n",
      "conv5_block1_3_bn\n",
      "conv5_block1_add\n",
      "conv5_block1_out\n",
      "conv5_block2_1_conv\n",
      "conv5_block2_1_bn\n",
      "conv5_block2_1_relu\n",
      "conv5_block2_2_conv\n",
      "conv5_block2_2_bn\n",
      "conv5_block2_2_relu\n",
      "conv5_block2_3_conv\n",
      "conv5_block2_3_bn\n",
      "conv5_block2_add\n",
      "conv5_block2_out\n",
      "conv5_block3_1_conv\n",
      "conv5_block3_1_bn\n",
      "conv5_block3_1_relu\n",
      "conv5_block3_2_conv\n",
      "conv5_block3_2_bn\n",
      "conv5_block3_2_relu\n",
      "conv5_block3_3_conv\n",
      "conv5_block3_3_bn\n",
      "conv5_block3_add\n",
      "conv5_block3_out\n",
      "avg_pool\n",
      "predictions\n"
     ]
    }
   ],
   "source": [
    "# Loading the model\n",
    "base_model = tf.keras.applications.ResNet50(weights='imagenet')\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    print(layer.name)\n",
    "# Specifying the input and output layers\n",
    "model = tf.keras.Model(inputs=base_model.input, outputs=base_model.get_layer('pool1_pool').output)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(images):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(images)\n",
    "    dataset = dataset.batch(300)\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_dataset(train_images)\n",
    "val_dataset = create_dataset(val_images)\n",
    "test_dataset = create_dataset(test_images)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = IncrementalPCA(n_components=100, batch_size=300)\n",
    "\n",
    "# Fit PCA to batch of datastet\n",
    "for batch in tqdm(train_dataset):\n",
    "    # Extract features\n",
    "    features = model.predict(batch)\n",
    "    # Flatten the features\n",
    "    features = features.reshape(features.shape[0], -1)\n",
    "    # Fit PCA to batch of features\n",
    "    pca.partial_fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the explained variance\n",
    "plt.plot(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(dataset, model, pca):\n",
    "    features = []\n",
    "    for batch in tqdm(dataset):\n",
    "        ft = model.predict(batch)\n",
    "        # Flatten the features\n",
    "        ft = ft.reshape(ft.shape[0], -1)\n",
    "        # Fit PCA to batch of features\n",
    "        ft = pca.transform(ft)\n",
    "        features.append(ft)\n",
    "    return np.vstack(features) # why do we need to stack the features? will .extend() work?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = extract_features(train_dataset, model, pca)\n",
    "features_val = extract_features(val_dataset, model, pca)\n",
    "features_test = extract_features(test_dataset, model, pca)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle frmi files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LH training fMRI data shape:\n",
      "(9841, 19004)\n",
      "(Training stimulus images × LH vertices)\n",
      "\n",
      "RH training fMRI data shape:\n",
      "(9841, 20544)\n",
      "(Training stimulus images × RH vertices)\n"
     ]
    }
   ],
   "source": [
    "fmri_dir = os.path.join(data_dir, 'training_split', 'training_fmri')\n",
    "lh_fmri = np.load(os.path.join(fmri_dir, 'lh_training_fmri.npy'))\n",
    "rh_fmri = np.load(os.path.join(fmri_dir, 'rh_training_fmri.npy'))\n",
    "\n",
    "print('LH training fMRI data shape:')\n",
    "print(lh_fmri.shape)\n",
    "print('(Training stimulus images × LH vertices)')\n",
    "\n",
    "print('\\nRH training fMRI data shape:')\n",
    "print(rh_fmri.shape)\n",
    "print('(Training stimulus images × RH vertices)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lh_fmri_train = lh_fmri[idxs_train]\n",
    "lh_fmri_val = lh_fmri[idxs_val]\n",
    "rh_fmri_train = rh_fmri[idxs_train]\n",
    "rh_fmri_val = rh_fmri[idxs_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del lh_fmri, rh_fmri"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_lh = LinearRegression().fit(features_train, lh_fmri_train)\n",
    "reg_rh = LinearRegression().fit(features_train, rh_fmri_train)\n",
    "# Use fitted linear regressions to predict the validation and test fMRI data\n",
    "lh_fmri_val_pred = reg_lh.predict(features_val)\n",
    "lh_fmri_test_pred = reg_lh.predict(features_test)\n",
    "rh_fmri_val_pred = reg_rh.predict(features_val)\n",
    "rh_fmri_test_pred = reg_rh.predict(features_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty correlation array of shape: (LH vertices)\n",
    "lh_correlation = np.zeros(lh_fmri_val_pred.shape[1])\n",
    "# Correlate each predicted LH vertex with the corresponding ground truth vertex\n",
    "for v in tqdm(range(lh_fmri_val_pred.shape[1])):\n",
    "    lh_correlation[v] = corr(lh_fmri_val_pred[:,v], lh_fmri_val[:,v])[0]\n",
    "\n",
    "# Empty correlation array of shape: (RH vertices)\n",
    "rh_correlation = np.zeros(rh_fmri_val_pred.shape[1])\n",
    "# Correlate each predicted RH vertex with the corresponding ground truth vertex\n",
    "for v in tqdm(range(rh_fmri_val_pred.shape[1])):\n",
    "    rh_correlation[v] = corr(rh_fmri_val_pred[:,v], rh_fmri_val[:,v])[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ROI classes mapping dictionaries\n",
    "roi_mapping_files = ['mapping_prf-visualrois.npy', 'mapping_floc-bodies.npy',\n",
    "    'mapping_floc-faces.npy', 'mapping_floc-places.npy',\n",
    "    'mapping_floc-words.npy', 'mapping_streams.npy']\n",
    "roi_name_maps = []\n",
    "for r in roi_mapping_files:\n",
    "    roi_name_maps.append(np.load(os.path.join(data_dir, 'roi_masks', r),\n",
    "        allow_pickle=True).item())\n",
    "\n",
    "# Load the ROI brain surface maps\n",
    "lh_challenge_roi_files = ['lh.prf-visualrois_challenge_space.npy',\n",
    "    'lh.floc-bodies_challenge_space.npy', 'lh.floc-faces_challenge_space.npy',\n",
    "    'lh.floc-places_challenge_space.npy', 'lh.floc-words_challenge_space.npy',\n",
    "    'lh.streams_challenge_space.npy']\n",
    "rh_challenge_roi_files = ['rh.prf-visualrois_challenge_space.npy',\n",
    "    'rh.floc-bodies_challenge_space.npy', 'rh.floc-faces_challenge_space.npy',\n",
    "    'rh.floc-places_challenge_space.npy', 'rh.floc-words_challenge_space.npy',\n",
    "    'rh.streams_challenge_space.npy']\n",
    "lh_challenge_rois = []\n",
    "rh_challenge_rois = []\n",
    "for r in range(len(lh_challenge_roi_files)):\n",
    "    lh_challenge_rois.append(np.load(os.path.join(data_dir, 'roi_masks',\n",
    "        lh_challenge_roi_files[r])))\n",
    "    rh_challenge_rois.append(np.load(os.path.join(data_dir, 'roi_masks',\n",
    "        rh_challenge_roi_files[r])))\n",
    "\n",
    "# Select the correlation results vertices of each ROI\n",
    "roi_names = []\n",
    "lh_roi_correlation = []\n",
    "rh_roi_correlation = []\n",
    "for r1 in range(len(lh_challenge_rois)):\n",
    "    for r2 in roi_name_maps[r1].items():\n",
    "        if r2[0] != 0: # zeros indicate to vertices falling outside the ROI of interest\n",
    "            roi_names.append(r2[1])\n",
    "            lh_roi_idx = np.where(lh_challenge_rois[r1] == r2[0])[0]\n",
    "            rh_roi_idx = np.where(rh_challenge_rois[r1] == r2[0])[0]\n",
    "            lh_roi_correlation.append(lh_correlation[lh_roi_idx])\n",
    "            rh_roi_correlation.append(rh_correlation[rh_roi_idx])\n",
    "roi_names.append('All vertices')\n",
    "lh_roi_correlation.append(lh_correlation)\n",
    "rh_roi_correlation.append(rh_correlation)\n",
    "\n",
    "# Create the plot\n",
    "lh_median_roi_correlation = [np.median(lh_roi_correlation[r])\n",
    "    for r in range(len(lh_roi_correlation))]\n",
    "rh_median_roi_correlation = [np.median(rh_roi_correlation[r])\n",
    "    for r in range(len(rh_roi_correlation))]\n",
    "plt.figure(figsize=(18,6))\n",
    "x = np.arange(len(roi_names))\n",
    "width = 0.30\n",
    "plt.bar(x - width/2, lh_median_roi_correlation, width, label='Left Hemisphere')\n",
    "plt.bar(x + width/2, rh_median_roi_correlation, width,\n",
    "    label='Right Hemishpere')\n",
    "plt.xlim(left=min(x)-.5, right=max(x)+.5)\n",
    "plt.ylim(bottom=0, top=1)\n",
    "plt.xlabel('ROIs')\n",
    "plt.xticks(ticks=x, labels=roi_names, rotation=60)\n",
    "plt.ylabel('Median Pearson\\'s $r$')\n",
    "plt.legend(frameon=True, loc=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algonauts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "365c028d1c840c93b0f470009ef966704a741407ca08d29f5bd0c0c6373757cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context manager to suppress stdoutm for annoying long printouts\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'vgg_imagenet'\n",
    "batch_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform = 'paperspace' #@param ['colab', 'paperspace'] {allow-input: true}\n",
    "\n",
    "if platform == 'jupyter_notebook':\n",
    "    challenge_data_dir = '../data/algonauts_2023_challenge_data'\n",
    "    exp_output_dir = f'../data/out/{experiment}'\n",
    "\n",
    "if platform == 'paperspace':\n",
    "    challenge_data_dir = '/storage/algonauts_2023_challenge_data'\n",
    "    exp_output_dir = f'/notebooks/data/out/{experiment}'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions not aligned with the current structuree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pca(model, train_dataset):\n",
    "    \n",
    "    features = []\n",
    "    for batch in train_dataset:\n",
    "        # Extract features\n",
    "        with HiddenPrints():\n",
    "            ft = model.predict(batch)\n",
    "        # Flatten the features\n",
    "        ft = ft.reshape(ft.shape[0], -1)\n",
    "        # Append to list\n",
    "        features.append(ft)\n",
    "    # Combine features from all batches\n",
    "    features = np.vstack(features)\n",
    "    n_components = 100\n",
    "    print('Features shape: ', features.shape, ' PCA Components: ', n_components)\n",
    "    # Fit PCA to combined features\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(features)\n",
    "    # print('plot the explained variance')\n",
    "    # plt.plot(pca.explained_variance_ratio_)\n",
    "    # plt.show()\n",
    "    # print('plot the cumulative explained variance')\n",
    "    # plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    # plt.show()\n",
    "    return pca\n",
    "\n",
    "def extract_and_transform_features(dataset, model, pca):\n",
    "    features = []\n",
    "    for batch in dataset:\n",
    "        with HiddenPrints():\n",
    "            ft = model.predict(batch)\n",
    "        # Flatten the features\n",
    "        ft = ft.reshape(ft.shape[0], -1)\n",
    "        # Fit PCA to batch of features\n",
    "        ft = pca.transform(ft)\n",
    "        features.append(ft)\n",
    "    return np.vstack(features)\n",
    "\n",
    "def load_datasets(dataset, transform_image):\n",
    "    train_val_imgs_paths = [os.path.join(dataset.train_img_dir, img_name) for img_name in dataset.training_img_list]\n",
    "    train_paths = [train_val_imgs_paths[i] for i in dataset.idxs_train]\n",
    "    val_paths = [train_val_imgs_paths[i] for i in dataset.idxs_val]\n",
    "    test_imgs_paths = [os.path.join(dataset.test_img_dir, img_name) for img_name in dataset.test_img_list]\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices(train_paths)\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices(val_paths)\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices(test_imgs_paths)\n",
    "\n",
    "    def load_image(image_path):\n",
    "        img = tf.io.read_file(image_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        return img\n",
    "    \n",
    "    def preprocess(ds):\n",
    "        ds = ds.map(load_image)\n",
    "        ds = ds.map(transform_image)\n",
    "        return ds\n",
    "    \n",
    "    train_ds = preprocess(train_ds).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds = preprocess(val_ds).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    test_ds = preprocess(test_ds).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "def save_predictions(lh_fmri_test_pred, rh_fmri_test_pred, subject_submission_dir):\n",
    "    lh_fmri_test_pred = lh_fmri_test_pred.astype(np.float32)\n",
    "    rh_fmri_test_pred = rh_fmri_test_pred.astype(np.float32)\n",
    "\n",
    "    np.save(os.path.join(subject_submission_dir, 'lh_pred_test.npy'), lh_fmri_test_pred)\n",
    "    np.save(os.path.join(subject_submission_dir, 'rh_pred_test.npy'), rh_fmri_test_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify model and appropriate image preprocessing steps\n",
    "Define a function to load and return the model, printing its layers.\n",
    "Due to memory constraints, we will delete the model from memory in each loop. Here we first load it, print its\n",
    "nodes and delete it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose to load a pretrained model from keras.applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(layer_name=None):\n",
    "    model = tf.keras.applications.VGG16(weights='imagenet', include_top=False)\n",
    "    # Slice the model to extract features from a specific layer\n",
    "    if layer_name is not None:\n",
    "        model = tf.keras.Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
    "    return model\n",
    "\n",
    "def transform_image(image):\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    image = tf.keras.applications.vgg16.preprocess_input(image)\n",
    "    return image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or choose to load a model from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'model.h5'\n",
    "\n",
    "def load_model(layer_name=None):\n",
    "    model = tf.keras.models.load_model(model_filename)\n",
    "    # Slice the model to extract features from a specific layer\n",
    "    if layer_name is not None:\n",
    "        model = tf.keras.Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
    "    return model\n",
    "\n",
    "def transform_image(image):\n",
    "    image = tf.image.resize(image, (227, 227))\n",
    "    image = tf.cast(image, tf.float32) / 127.5 - 1.0\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    variance = [0.229, 0.224, 0.225]\n",
    "    normalization_layer = Normalization(mean=mean, variance=variance)\n",
    "    image = normalization_layer(image)\n",
    "    return image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model()\n",
    "print(*(layer.name for layer in model.layers), sep=' -> ')\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select layers and subjects\n",
    "Now let's define which layer(s) we will pick from, and which subject(s) to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = ['block1_pool']\n",
    "subjects = [\n",
    "    1, \n",
    "    # 2, 3, 4, 5, 6, 7, 8\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.algonauts.evaluators import correlations as corr\n",
    "from src.algonauts.data_processors.nsd_dataset import NSDDataset\n",
    "\n",
    "for layer_name in layers:\n",
    "    print(f'Running for layer {layer_name}')\n",
    "    for subj in subjects:\n",
    "        print(f'Running for subject {subj}')\n",
    "\n",
    "        # Set data directories based on parameters\n",
    "        output_dir = f'{exp_output_dir}/{layer_name}'\n",
    "        dataset = NSDDataset(challenge_data_dir, output_dir, subj)\n",
    "\n",
    "        print('Loading datasets...')\n",
    "        train_ds, val_ds, test_ds = load_datasets(dataset, transform_image)\n",
    "        print('Datasets loaded')\n",
    "\n",
    "        # Load model for the iteration\n",
    "        model = load_model(layer_name)\n",
    "        \n",
    "        # Train PCA\n",
    "        print('Training PCA...')\n",
    "        pca = train_pca(model, train_ds)\n",
    "        print('PCA over')\n",
    "\n",
    "        # Extract and transform features\n",
    "        print('Extracting and transforming features...')\n",
    "        train_features = extract_and_transform_features(train_ds, model, pca)\n",
    "        val_features = extract_and_transform_features(val_ds, model, pca)\n",
    "        test_features = extract_and_transform_features(test_ds, model, pca)\n",
    "        print('Features extracted and transformed')\n",
    "\n",
    "        # Delete model to free up memory\n",
    "        del model, pca\n",
    "\n",
    "        # Fit regression\n",
    "        print('Fitting regression...')\n",
    "        reg_lh = LinearRegression().fit(train_features, dataset.lh_fmri_train)\n",
    "        reg_rh = LinearRegression().fit(train_features, dataset.rh_fmri_train)\n",
    "        print('Regression fitted')\n",
    "        \n",
    "        # Use fitted linear regressions to predict the validation and test fMRI data\n",
    "        print('Predicting fMRI data...')\n",
    "        lh_fmri_val_pred = reg_lh.predict(val_features)\n",
    "        lh_fmri_test_pred = reg_lh.predict(test_features)\n",
    "        rh_fmri_val_pred = reg_rh.predict(val_features)\n",
    "        rh_fmri_test_pred = reg_rh.predict(test_features)\n",
    "        print('fMRI data predicted')\n",
    "        # Calculate correlations for each hemispher\n",
    "        print('Calculating correlations...')\n",
    "        lh_correlation = corr.calculate_correlation(lh_fmri_val_pred, dataset.lh_fmri_val)\n",
    "        rh_correlation = corr.calculate_correlation(rh_fmri_val_pred, dataset.rh_fmri_val)  \n",
    "        print('Correlations calculated')\n",
    "    \n",
    "        corr.plot_and_write_correlations(dataset, lh_correlation, rh_correlation, exp_output_dir, layer_name, subj)\n",
    "        # Save test predictions\n",
    "        np.save(os.path.join(dataset.subject_submission_dir, 'lh_pred_test.npy'), lh_fmri_test_pred)\n",
    "        np.save(os.path.join(dataset.subject_submission_dir, 'rh_pred_test.npy'), rh_fmri_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"LH\": {\n",
      "    \"V1v\": {\n",
      "      \"layer\": \"block2_pool\",\n",
      "      \"value\": 0.5905791030486138\n",
      "    },\n",
      "    \"V1d\": {\n",
      "      \"layer\": \"block2_pool\",\n",
      "      \"value\": 0.5990486552463073\n",
      "    },\n",
      "    \"V2v\": {\n",
      "      \"layer\": \"block3_pool\",\n",
      "      \"value\": 0.5282678641036374\n",
      "    },\n",
      "    \"V2d\": {\n",
      "      \"layer\": \"block3_pool\",\n",
      "      \"value\": 0.5175909098720242\n",
      "    },\n",
      "    \"V3v\": {\n",
      "      \"layer\": \"block3_pool\",\n",
      "      \"value\": 0.5141076787726454\n",
      "    },\n",
      "    \"V3d\": {\n",
      "      \"layer\": \"block4_pool\",\n",
      "      \"value\": 0.5065629532466085\n",
      "    },\n",
      "    \"hV4\": {\n",
      "      \"layer\": \"block4_pool\",\n",
      "      \"value\": 0.44591214364383464\n",
      "    },\n",
      "    \"EBA\": {\n",
      "      \"layer\": \"block5_pool\",\n",
      "      \"value\": 0.4846002619845918\n",
      "    },\n",
      "    \"FBA-1\": {\n",
      "      \"layer\": \"block5_pool\",\n",
      "      \"value\": 0.4077181141971304\n",
      "    },\n",
      "    \"FBA-2\": {\n",
      "      \"layer\": null,\n",
      "      \"value\": null\n",
      "    },\n",
      "    \"mTL-bodies\": {\n",
      "      \"layer\": null,\n",
      "      \"value\": null\n",
      "    },\n",
      "    \"OFA\": {\n",
      "      \"layer\": \"block4_pool\",\n",
      "      \"value\": 0.43446271378361234\n",
      "    },\n",
      "    \"FFA-1\": {\n",
      "      \"layer\": \"block5_pool\",\n",
      "      \"value\": 0.47813203915081814\n",
      "    },\n",
      "    \"FFA-2\": {\n",
      "      \"layer\": null,\n",
      "      \"value\": null\n",
      "    },\n",
      "    \"mTL-faces\": {\n",
      "      \"layer\": null,\n",
      "      \"value\": null\n",
      "    },\n",
      "    \"aTL-faces\": {\n",
      "      \"layer\": null,\n",
      "      \"value\": null\n",
      "    },\n",
      "    \"OPA\": {\n",
      "      \"layer\": \"block5_pool\",\n",
      "      \"value\": 0.4635288543601275\n",
      "    },\n",
      "    \"PPA\": {\n",
      "      \"layer\": \"block5_pool\",\n",
      "      \"value\": 0.45243649416808523\n",
      "    },\n",
      "    \"RSC\": {\n",
      "      \"layer\": \"block5_pool\",\n",
      "      \"value\": 0.5647854640407909\n",
      "    },\n",
      "    \"OWFA\": {\n",
      "      \"layer\": \"block4_pool\",\n",
      "      \"value\": 0.4403960309667769\n",
      "    },\n",
      "    \"VWFA-1\": {\n",
      "      \"layer\": \"block5_pool\",\n",
      "      \"value\": 0.39514660200442236\n",
      "    },\n",
      "    \"VWFA-2\": {\n",
      "      \"layer\": \"block5_pool\",\n",
      "      \"value\": 0.29124631654009503\n",
      "    },\n",
      "    \"mfs-words\": {\n",
      "      \"layer\": \"block5_pool\",\n",
      "      \"value\": 0.33307782920377954\n",
      "    },\n",
      "    \"mTL-words\": {\n",
      "      \"layer\": null,\n",
      "      \"value\": null\n",
      "    },\n",
      "    \"early\": {\n",
      "      \"layer\": \"block3_pool\",\n",
      "      \"value\": 0.48535367849473543\n",
      "    },\n",
      "    \"midventral\": {\n",
      "      \"layer\": \"block4_pool\",\n",
      "      \"value\": 0.4509723722235723\n",
      "    },\n",
      "    \"midlateral\": {\n",
      "      \"layer\": \"block4_pool\",\n",
      "      \"value\": 0.4119562613559015\n",
      "    },\n",
      "    \"midparietal\": {\n",
      "      \"layer\": \"block4_pool\",\n",
      "      \"value\": 0.47420258748392696\n",
      "    },\n",
      "    \"ventral\": {\n",
      "      \"layer\": \"block5_pool\",\n",
      "      \"value\": 0.3921019226381709\n",
      "    },\n",
      "    \"lateral\": {\n",
      "      \"layer\": \"block5_pool\",\n",
      "      \"value\": 0.4800048953552641\n",
      "    },\n",
      "    \"parietal\": {\n",
      "      \"layer\": \"block5_pool\",\n",
      "      \"value\": 0.40883674192451813\n",
      "    },\n",
      "    \"All vertices\": {\n",
      "      \"layer\": \"block4_pool\",\n",
      "      \"value\": 0.4139067778985958\n",
      "    }\n",
      "  },\n",
      "  \"RH\": {\n",
      "    \"V1v\": {\n",
      "      \"layer\": \"block2_pool\",\n",
      "      \"value\": 0.5805181756669613\n",
      "    },\n",
      "    \"V1d\": {\n",
      "      \"layer\": \"block2_pool\",\n",
      "      \"value\": 0.5851275998692367\n",
      "    },\n",
      "    \"V2v\": {\n",
      "      \"layer\": \"block3_pool\",\n",
      "      \"value\": 0.5686585857121095\n",
      "    },\n",
      "    \"V2d\": {\n",
      "      \"layer\": \"block4_pool\",\n",
      "      \"value\": 0.5609733755006775\n",
      "    },\n",
      "    \"V3v\": {\n",
      "      \"layer\": \"block4_pool\",\n",
      "      \"value\": 0.5119499674719273\n",
      "    },\n",
      "    \"V3d\": {\n",
      "      \"layer\": \"block4_pool\",\n",
      "      \"value\": 0.5151066375578838\n",
      "    },\n",
      "    \"hV4\": {\n",
      "      \"layer\": \"block4_pool\",\n",
      "      \"value\": 0.48961573812595627\n",
      "    },\n",
      "    \"EBA\": {\n",
      "      \"layer\": \"block5_pool\",\n",
      "      \"value\": 0.4822663700000454\n",
      "    },\n",
      "    \"FBA-1\": {\n",
      "      \"layer\": \"block4_pool\",\n",
      "      \"value\": 0.36988531367244193\n",
      "    },\n",
      "    \"FBA-2\": {\n",
      "      \"layer\": \"block5_pool\",\n",
      "      \"value\": 0.4628685204806653\n",
      "    },\n",
      "    \"mTL-bodies\": {\n",
      "      \"layer\": null,\n",
      "      \"value\": null\n",
      "    },\n",
      "    \"OFA\": {\n",
      "      \"layer\": \"block4_pool\",\n",
      "      \"value\": 0.40641462406129836\n",
      "    },\n",
      "    \"FFA-1\": {\n",
      "      \"layer\": \"block5_pool\",\n",
      "      \"value\": 0.4267493564860309\n",
      "    },\n",
      "    \"FFA-2\": {\n",
      "      \"layer\": \"block5_pool\",\n",
      "      \"value\": 0.5508115593284029\n",
      "    },\n",
      "    \"mTL-faces\": {\n",
      "      \"layer\": null,\n",
      "      \"value\": null\n",
      "    },\n",
      "    \"aTL-faces\": {\n",
      "      \"layer\": null,\n",
      "      \"value\": null\n",
      "    },\n",
      "    \"OPA\": {\n",
      "      \"layer\": \"block5_pool\",\n",
      "      \"value\": 0.42695924580023425\n",
      "    },\n",
      "    \"PPA\": {\n",
      "      \"layer\": \"block5_pool\",\n",
      "      \"value\": 0.5168665400732207\n",
      "    },\n",
      "    \"RSC\": {\n",
      "      \"layer\": \"block5_pool\",\n",
      "      \"value\": 0.5307699674909476\n",
      "    },\n",
      "    \"OWFA\": {\n",
      "      \"layer\": \"block4_pool\",\n",
      "      \"value\": 0.38254402945690213\n",
      "    },\n",
      "    \"VWFA-1\": {\n",
      "      \"layer\": \"block5_pool\",\n",
      "      \"value\": 0.47109845382754123\n",
      "    },\n",
      "    \"VWFA-2\": {\n",
      "      \"layer\": \"block5_pool\",\n",
      "      \"value\": 0.3554232780325516\n",
      "    },\n",
      "    \"mfs-words\": {\n",
      "      \"layer\": null,\n",
      "      \"value\": null\n",
      "    },\n",
      "    \"mTL-words\": {\n",
      "      \"layer\": null,\n",
      "      \"value\": null\n",
      "    },\n",
      "    \"early\": {\n",
      "      \"layer\": \"block3_pool\",\n",
      "      \"value\": 0.4929368762787638\n",
      "    },\n",
      "    \"midventral\": {\n",
      "      \"layer\": \"block4_pool\",\n",
      "      \"value\": 0.4357688748101849\n",
      "    },\n",
      "    \"midlateral\": {\n",
      "      \"layer\": \"block4_pool\",\n",
      "      \"value\": 0.4218590697034337\n",
      "    },\n",
      "    \"midparietal\": {\n",
      "      \"layer\": \"block4_pool\",\n",
      "      \"value\": 0.4267432791472524\n",
      "    },\n",
      "    \"ventral\": {\n",
      "      \"layer\": \"block5_pool\",\n",
      "      \"value\": 0.4541174006589327\n",
      "    },\n",
      "    \"lateral\": {\n",
      "      \"layer\": \"block5_pool\",\n",
      "      \"value\": 0.4399956095988502\n",
      "    },\n",
      "    \"parietal\": {\n",
      "      \"layer\": \"block5_pool\",\n",
      "      \"value\": 0.3688299188700996\n",
      "    },\n",
      "    \"All vertices\": {\n",
      "      \"layer\": \"block4_pool\",\n",
      "      \"value\": 0.41940475057257726\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def find_best_correlations(exp_output_dir, subject_id):\n",
    "    json_path = f'{exp_output_dir}/results.json'\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    layers_names_with_filtered_subjects = [\n",
    "        (layer[\"layer_name\"], subj)\n",
    "        for layer in data[\"Layers\"] \n",
    "        for subj in layer[\"Subjects\"]\n",
    "        if subj[\"subject\"] == subject_id\n",
    "    ]\n",
    "\n",
    "    def max_layer_value(subj_data, hemi, roi):\n",
    "        max_layer, max_value = max(\n",
    "            (\n",
    "                (layer_name, roi_corr.get(roi))\n",
    "                for layer_name, subj in subj_data\n",
    "                for roi_corr in [subj[f\"{hemi}_median_roi_correlation\"]]\n",
    "                if roi_corr.get(roi) is not None\n",
    "            ),\n",
    "            key=lambda x: x[1],\n",
    "            default=(None, None),\n",
    "        )\n",
    "        return max_layer, max_value\n",
    "\n",
    "    roi_names = list(layers_names_with_filtered_subjects[0][1][\"LH_median_roi_correlation\"].keys())\n",
    "\n",
    "    best_correlation = {\n",
    "        hemi: {\n",
    "            roi: {\n",
    "                \"layer\": max_layer_value(layers_names_with_filtered_subjects, hemi, roi)[0],\n",
    "                \"value\": max_layer_value(layers_names_with_filtered_subjects, hemi, roi)[1],\n",
    "            }\n",
    "            for roi in roi_names\n",
    "        }\n",
    "        for hemi in [\"LH\", \"RH\"]\n",
    "    }\n",
    "\n",
    "    return best_correlation\n",
    "\n",
    "\n",
    "subj = 1\n",
    "result = find_best_correlations(exp_output_dir, subj)\n",
    "print(json.dumps(result, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "365c028d1c840c93b0f470009ef966704a741407ca08d29f5bd0c0c6373757cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

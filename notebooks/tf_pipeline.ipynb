{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Run Algonauts Challenge 2023 Pipeline\n",
    "\n",
    "Here you can run the challenge pipeline (ie. generate predictions) using a pretrained model from keras.applications, or load a model from a model file.\n",
    "This notebook will output challenge submission files, as well as correlation graphs and a results.json file\n",
    "containing all correlation results for all layers, subjects and ROIs.\n",
    "You can also print best layers per ROI and generate submission files merged from the best layers in addition\n",
    "to predictions from single layers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.algonauts.data_processors.image_transforms\n",
    "from src.algonauts.models import model_loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define experiment name here. Output files will be generated under `{base_folder}/data/out/{experiment}`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'alexnet_coco'\n",
    "batch_size = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pick environment here. If running locally, pick 'jupyter_notebook'."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "platform = 'jupyter_notebook' #@param ['colab', 'paperspace'] {allow-input: true}\n",
    "\n",
    "if platform == 'jupyter_notebook':\n",
    "    data_dir = '../data'\n",
    "    challenge_data_dir = f'{data_dir}/algonauts_2023_challenge_data'\n",
    "    exp_output_dir = f'{data_dir}/out/{experiment}'\n",
    "\n",
    "if platform == 'paperspace':\n",
    "    data_dir = '.'\n",
    "    challenge_data_dir = '/storage/algonauts_2023_challenge_data'\n",
    "    exp_output_dir = f'/notebooks/data/out/{experiment}'\n",
    "\n",
    "if platform == 'colab':\n",
    "    data_dir = '/content/drive/MyDrive/algonauts23/data'\n",
    "    challenge_data_dir = f'{data_dir}/algonauts_2023_challenge_data'\n",
    "    exp_output_dir = f'{data_dir}/out/{experiment}'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify model and appropriate image preprocessing steps\n",
    "Define a function to load and return the model, printing its layers.\n",
    "Here we first load the model and print its nodes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose to load a pretrained model from keras.applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loader = lambda: model_loaders.load_vgg16()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or choose to load a model from file\n",
    "\n",
    "We use alexnet_coco as an example. Change model filename to load a different model. You must also pick or\n",
    "implement an image_transform for the model's needs that will be used on the stimulus images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = f'{data_dir}/models/alexnet_coco.h5'\n",
    "transform_image = src.algonauts.data_processors.image_transforms.transform_alexnet\n",
    "model_loader = lambda: model_loaders.load_from_file(model_filename, transform_image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model, _ = model_loader()\n",
    "print(*(layer.name for layer in model.layers), sep=' -> ')\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select layers and subjects\n",
    "Now let's define which layer(s) we will pick from, and which subject(s) to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = ['block1_pool', 'block2_pool', 'block3_pool', 'block4_pool', 'block5_pool']\n",
    "subjects = [\n",
    "    1, 2, 3, 4, 5, 6, 7, 8\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from src.algonauts.pipelines.tf_pipeline import run_tf_pipeline\n",
    "\n",
    "run_tf_pipeline(batch_size=batch_size, model_loader=model_loader, layers=layers, subjects=subjects,\n",
    "                challenge_data_dir=challenge_data_dir,\n",
    "                exp_output_dir=exp_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Merge predictions from best layers for each ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from src.algonauts.evaluators.predictions import merge_predictions_for_all_subjects\n",
    "merge_predictions_for_all_subjects(subjects, challenge_data_dir, exp_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Print best layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from src.algonauts.evaluators.correlations import find_best_correlations\n",
    "\n",
    "subj = 1\n",
    "result = find_best_correlations(f'{exp_output_dir}/results.json', subj)\n",
    "print(json.dumps(result, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "365c028d1c840c93b0f470009ef966704a741407ca08d29f5bd0c0c6373757cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

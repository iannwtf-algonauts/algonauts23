{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context manager to suppress stdoutm for annoying long printouts\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'tf_resnet50'\n",
    "batch_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform = 'paperspace' #@param ['colab', 'paperspace'] {allow-input: true}\n",
    "\n",
    "if platform == 'jupyter_notebook':\n",
    "    challenge_data_dir = '../data/algonauts_2023_challenge_data'\n",
    "    exp_output_dir = f'../data/out/{experiment}'\n",
    "\n",
    "if platform == 'paperspace':\n",
    "    challenge_data_dir = '/storage/algonauts_2023_challenge_data'\n",
    "    exp_output_dir = f'/notebooks/data/out/{experiment}'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions not aligned with the current structuree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pca(model, train_dataset):\n",
    "    \n",
    "    features = []\n",
    "    for batch in train_dataset:\n",
    "        # Extract features\n",
    "        with HiddenPrints():\n",
    "            ft = model.predict(batch)\n",
    "        # Flatten the features\n",
    "        ft = ft.reshape(ft.shape[0], -1)\n",
    "        # Append to list\n",
    "        features.append(ft)\n",
    "    # Combine features from all batches\n",
    "    del model\n",
    "    features = np.vstack(features)\n",
    "    # Fit PCA to combined features\n",
    "    pca = PCA(n_components=100)\n",
    "    pca.fit(features)\n",
    "    # print('plot the explained variance')\n",
    "    # plt.plot(pca.explained_variance_ratio_)\n",
    "    # plt.show()\n",
    "    # print('plot the cumulative explained variance')\n",
    "    # plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    # plt.show()\n",
    "    return pca\n",
    "\n",
    "def extract_and_transform_features(dataset, model, pca):\n",
    "    features = []\n",
    "    for batch in dataset:\n",
    "        with HiddenPrints():\n",
    "            ft = model.predict(batch)\n",
    "        # Flatten the features\n",
    "        ft = ft.reshape(ft.shape[0], -1)\n",
    "        # Fit PCA to batch of features\n",
    "        ft = pca.transform(ft)\n",
    "        features.append(ft)\n",
    "    return np.vstack(features)\n",
    "\n",
    "def load_datasets(dataset, image_size, normalise_dataset):\n",
    "    train_val_imgs_paths = [os.path.join(dataset.train_img_dir, img_name) for img_name in dataset.training_img_list]\n",
    "    train_paths = [train_val_imgs_paths[i] for i in dataset.idxs_train]\n",
    "    val_paths = [train_val_imgs_paths[i] for i in dataset.idxs_val]\n",
    "    test_imgs_paths = [os.path.join(dataset.test_img_dir, img_name) for img_name in dataset.test_img_list]\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices(train_paths)\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices(val_paths)\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices(test_imgs_paths)\n",
    "\n",
    "    def load_image(image_path):\n",
    "        # load image file\n",
    "        img = tf.io.read_file(image_path)\n",
    "        # decode image\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        # resize image to a fixed shape\n",
    "        img = tf.image.resize(img, image_size)\n",
    "        return img\n",
    "    \n",
    "    def process(ds):\n",
    "        ds = ds.map(load_image)\n",
    "        ds = ds.map(normalise_dataset)\n",
    "        ds = ds.batch(batch_size)\n",
    "        ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "        return ds\n",
    "    \n",
    "    train_ds = process(train_ds)\n",
    "    val_ds = process(val_ds)\n",
    "    test_ds = process(test_ds)\n",
    "\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "def save_predictions(lh_fmri_test_pred, rh_fmri_test_pred, subject_submission_dir):\n",
    "    lh_fmri_test_pred = lh_fmri_test_pred.astype(np.float32)\n",
    "    rh_fmri_test_pred = rh_fmri_test_pred.astype(np.float32)\n",
    "\n",
    "    np.save(os.path.join(subject_submission_dir, 'lh_pred_test.npy'), lh_fmri_test_pred)\n",
    "    np.save(os.path.join(subject_submission_dir, 'rh_pred_test.npy'), rh_fmri_test_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify model and appropriate image preprocessing steps\n",
    "Define a function to load and return the model, printing its layers.\n",
    "Due to memory constraints, we will delete the model from memory in each loop. Here we first load it, print its\n",
    "nodes and delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb4_notop.h5\n",
      "71686520/71686520 [==============================] - 1s 0us/step\n",
      "input_1 -> rescaling -> normalization -> tf.math.truediv -> stem_conv_pad -> stem_conv -> stem_bn -> stem_activation -> block1a_dwconv -> block1a_bn -> block1a_activation -> block1a_se_squeeze -> block1a_se_reshape -> block1a_se_reduce -> block1a_se_expand -> block1a_se_excite -> block1a_project_conv -> block1a_project_bn -> block1b_dwconv -> block1b_bn -> block1b_activation -> block1b_se_squeeze -> block1b_se_reshape -> block1b_se_reduce -> block1b_se_expand -> block1b_se_excite -> block1b_project_conv -> block1b_project_bn -> block1b_drop -> block1b_add -> block2a_expand_conv -> block2a_expand_bn -> block2a_expand_activation -> block2a_dwconv_pad -> block2a_dwconv -> block2a_bn -> block2a_activation -> block2a_se_squeeze -> block2a_se_reshape -> block2a_se_reduce -> block2a_se_expand -> block2a_se_excite -> block2a_project_conv -> block2a_project_bn -> block2b_expand_conv -> block2b_expand_bn -> block2b_expand_activation -> block2b_dwconv -> block2b_bn -> block2b_activation -> block2b_se_squeeze -> block2b_se_reshape -> block2b_se_reduce -> block2b_se_expand -> block2b_se_excite -> block2b_project_conv -> block2b_project_bn -> block2b_drop -> block2b_add -> block2c_expand_conv -> block2c_expand_bn -> block2c_expand_activation -> block2c_dwconv -> block2c_bn -> block2c_activation -> block2c_se_squeeze -> block2c_se_reshape -> block2c_se_reduce -> block2c_se_expand -> block2c_se_excite -> block2c_project_conv -> block2c_project_bn -> block2c_drop -> block2c_add -> block2d_expand_conv -> block2d_expand_bn -> block2d_expand_activation -> block2d_dwconv -> block2d_bn -> block2d_activation -> block2d_se_squeeze -> block2d_se_reshape -> block2d_se_reduce -> block2d_se_expand -> block2d_se_excite -> block2d_project_conv -> block2d_project_bn -> block2d_drop -> block2d_add -> block3a_expand_conv -> block3a_expand_bn -> block3a_expand_activation -> block3a_dwconv_pad -> block3a_dwconv -> block3a_bn -> block3a_activation -> block3a_se_squeeze -> block3a_se_reshape -> block3a_se_reduce -> block3a_se_expand -> block3a_se_excite -> block3a_project_conv -> block3a_project_bn -> block3b_expand_conv -> block3b_expand_bn -> block3b_expand_activation -> block3b_dwconv -> block3b_bn -> block3b_activation -> block3b_se_squeeze -> block3b_se_reshape -> block3b_se_reduce -> block3b_se_expand -> block3b_se_excite -> block3b_project_conv -> block3b_project_bn -> block3b_drop -> block3b_add -> block3c_expand_conv -> block3c_expand_bn -> block3c_expand_activation -> block3c_dwconv -> block3c_bn -> block3c_activation -> block3c_se_squeeze -> block3c_se_reshape -> block3c_se_reduce -> block3c_se_expand -> block3c_se_excite -> block3c_project_conv -> block3c_project_bn -> block3c_drop -> block3c_add -> block3d_expand_conv -> block3d_expand_bn -> block3d_expand_activation -> block3d_dwconv -> block3d_bn -> block3d_activation -> block3d_se_squeeze -> block3d_se_reshape -> block3d_se_reduce -> block3d_se_expand -> block3d_se_excite -> block3d_project_conv -> block3d_project_bn -> block3d_drop -> block3d_add -> block4a_expand_conv -> block4a_expand_bn -> block4a_expand_activation -> block4a_dwconv_pad -> block4a_dwconv -> block4a_bn -> block4a_activation -> block4a_se_squeeze -> block4a_se_reshape -> block4a_se_reduce -> block4a_se_expand -> block4a_se_excite -> block4a_project_conv -> block4a_project_bn -> block4b_expand_conv -> block4b_expand_bn -> block4b_expand_activation -> block4b_dwconv -> block4b_bn -> block4b_activation -> block4b_se_squeeze -> block4b_se_reshape -> block4b_se_reduce -> block4b_se_expand -> block4b_se_excite -> block4b_project_conv -> block4b_project_bn -> block4b_drop -> block4b_add -> block4c_expand_conv -> block4c_expand_bn -> block4c_expand_activation -> block4c_dwconv -> block4c_bn -> block4c_activation -> block4c_se_squeeze -> block4c_se_reshape -> block4c_se_reduce -> block4c_se_expand -> block4c_se_excite -> block4c_project_conv -> block4c_project_bn -> block4c_drop -> block4c_add -> block4d_expand_conv -> block4d_expand_bn -> block4d_expand_activation -> block4d_dwconv -> block4d_bn -> block4d_activation -> block4d_se_squeeze -> block4d_se_reshape -> block4d_se_reduce -> block4d_se_expand -> block4d_se_excite -> block4d_project_conv -> block4d_project_bn -> block4d_drop -> block4d_add -> block4e_expand_conv -> block4e_expand_bn -> block4e_expand_activation -> block4e_dwconv -> block4e_bn -> block4e_activation -> block4e_se_squeeze -> block4e_se_reshape -> block4e_se_reduce -> block4e_se_expand -> block4e_se_excite -> block4e_project_conv -> block4e_project_bn -> block4e_drop -> block4e_add -> block4f_expand_conv -> block4f_expand_bn -> block4f_expand_activation -> block4f_dwconv -> block4f_bn -> block4f_activation -> block4f_se_squeeze -> block4f_se_reshape -> block4f_se_reduce -> block4f_se_expand -> block4f_se_excite -> block4f_project_conv -> block4f_project_bn -> block4f_drop -> block4f_add -> block5a_expand_conv -> block5a_expand_bn -> block5a_expand_activation -> block5a_dwconv -> block5a_bn -> block5a_activation -> block5a_se_squeeze -> block5a_se_reshape -> block5a_se_reduce -> block5a_se_expand -> block5a_se_excite -> block5a_project_conv -> block5a_project_bn -> block5b_expand_conv -> block5b_expand_bn -> block5b_expand_activation -> block5b_dwconv -> block5b_bn -> block5b_activation -> block5b_se_squeeze -> block5b_se_reshape -> block5b_se_reduce -> block5b_se_expand -> block5b_se_excite -> block5b_project_conv -> block5b_project_bn -> block5b_drop -> block5b_add -> block5c_expand_conv -> block5c_expand_bn -> block5c_expand_activation -> block5c_dwconv -> block5c_bn -> block5c_activation -> block5c_se_squeeze -> block5c_se_reshape -> block5c_se_reduce -> block5c_se_expand -> block5c_se_excite -> block5c_project_conv -> block5c_project_bn -> block5c_drop -> block5c_add -> block5d_expand_conv -> block5d_expand_bn -> block5d_expand_activation -> block5d_dwconv -> block5d_bn -> block5d_activation -> block5d_se_squeeze -> block5d_se_reshape -> block5d_se_reduce -> block5d_se_expand -> block5d_se_excite -> block5d_project_conv -> block5d_project_bn -> block5d_drop -> block5d_add -> block5e_expand_conv -> block5e_expand_bn -> block5e_expand_activation -> block5e_dwconv -> block5e_bn -> block5e_activation -> block5e_se_squeeze -> block5e_se_reshape -> block5e_se_reduce -> block5e_se_expand -> block5e_se_excite -> block5e_project_conv -> block5e_project_bn -> block5e_drop -> block5e_add -> block5f_expand_conv -> block5f_expand_bn -> block5f_expand_activation -> block5f_dwconv -> block5f_bn -> block5f_activation -> block5f_se_squeeze -> block5f_se_reshape -> block5f_se_reduce -> block5f_se_expand -> block5f_se_excite -> block5f_project_conv -> block5f_project_bn -> block5f_drop -> block5f_add -> block6a_expand_conv -> block6a_expand_bn -> block6a_expand_activation -> block6a_dwconv_pad -> block6a_dwconv -> block6a_bn -> block6a_activation -> block6a_se_squeeze -> block6a_se_reshape -> block6a_se_reduce -> block6a_se_expand -> block6a_se_excite -> block6a_project_conv -> block6a_project_bn -> block6b_expand_conv -> block6b_expand_bn -> block6b_expand_activation -> block6b_dwconv -> block6b_bn -> block6b_activation -> block6b_se_squeeze -> block6b_se_reshape -> block6b_se_reduce -> block6b_se_expand -> block6b_se_excite -> block6b_project_conv -> block6b_project_bn -> block6b_drop -> block6b_add -> block6c_expand_conv -> block6c_expand_bn -> block6c_expand_activation -> block6c_dwconv -> block6c_bn -> block6c_activation -> block6c_se_squeeze -> block6c_se_reshape -> block6c_se_reduce -> block6c_se_expand -> block6c_se_excite -> block6c_project_conv -> block6c_project_bn -> block6c_drop -> block6c_add -> block6d_expand_conv -> block6d_expand_bn -> block6d_expand_activation -> block6d_dwconv -> block6d_bn -> block6d_activation -> block6d_se_squeeze -> block6d_se_reshape -> block6d_se_reduce -> block6d_se_expand -> block6d_se_excite -> block6d_project_conv -> block6d_project_bn -> block6d_drop -> block6d_add -> block6e_expand_conv -> block6e_expand_bn -> block6e_expand_activation -> block6e_dwconv -> block6e_bn -> block6e_activation -> block6e_se_squeeze -> block6e_se_reshape -> block6e_se_reduce -> block6e_se_expand -> block6e_se_excite -> block6e_project_conv -> block6e_project_bn -> block6e_drop -> block6e_add -> block6f_expand_conv -> block6f_expand_bn -> block6f_expand_activation -> block6f_dwconv -> block6f_bn -> block6f_activation -> block6f_se_squeeze -> block6f_se_reshape -> block6f_se_reduce -> block6f_se_expand -> block6f_se_excite -> block6f_project_conv -> block6f_project_bn -> block6f_drop -> block6f_add -> block6g_expand_conv -> block6g_expand_bn -> block6g_expand_activation -> block6g_dwconv -> block6g_bn -> block6g_activation -> block6g_se_squeeze -> block6g_se_reshape -> block6g_se_reduce -> block6g_se_expand -> block6g_se_excite -> block6g_project_conv -> block6g_project_bn -> block6g_drop -> block6g_add -> block6h_expand_conv -> block6h_expand_bn -> block6h_expand_activation -> block6h_dwconv -> block6h_bn -> block6h_activation -> block6h_se_squeeze -> block6h_se_reshape -> block6h_se_reduce -> block6h_se_expand -> block6h_se_excite -> block6h_project_conv -> block6h_project_bn -> block6h_drop -> block6h_add -> block7a_expand_conv -> block7a_expand_bn -> block7a_expand_activation -> block7a_dwconv -> block7a_bn -> block7a_activation -> block7a_se_squeeze -> block7a_se_reshape -> block7a_se_reduce -> block7a_se_expand -> block7a_se_excite -> block7a_project_conv -> block7a_project_bn -> block7b_expand_conv -> block7b_expand_bn -> block7b_expand_activation -> block7b_dwconv -> block7b_bn -> block7b_activation -> block7b_se_squeeze -> block7b_se_reshape -> block7b_se_reduce -> block7b_se_expand -> block7b_se_excite -> block7b_project_conv -> block7b_project_bn -> block7b_drop -> block7b_add -> top_conv -> top_bn -> top_activation\n"
     ]
    }
   ],
   "source": [
    "def load_model():\n",
    "    # Take models from here https://keras.io/api/applications/\n",
    "    model = tf.keras.applications.EfficientNetB4(weights='imagenet', include_top=False)\n",
    "    return model\n",
    "\n",
    "normalise_dataset = tf.keras.applications.efficientnet.preprocess_input\n",
    "image_size = (300, 300)\n",
    "\n",
    "model = load_model()\n",
    "print(*(layer.name for layer in model.layers), sep=' -> ')\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select layers and subjects\n",
    "Now let's define which layer(s) we will pick from, and which subject(s) to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = ['block6d_bn']\n",
    "subjects = [1, 2, 3, 4, 5, 6, 7, 8]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for layer block6d_bn\n",
      "Running for subject 1\n",
      "Training images: 9841\n",
      "Test images: 159\n",
      "LH training fMRI data shape:\n",
      "(9841, 19004)\n",
      "(Training stimulus images × LH vertices)\n",
      "\n",
      "RH training fMRI data shape:\n",
      "(9841, 20544)\n",
      "(Training stimulus images × RH vertices)\n",
      "Training images: 9841\n",
      "Test images: 159\n",
      "Training stimulus images: 8856\n",
      "\n",
      "Validation stimulus images: 985\n",
      "\n",
      "Test stimulus images: 159\n",
      "Loading datasets...\n",
      "Datasets loaded\n",
      "Training PCA...\n",
      "PCA over\n",
      "Extracting and transforming features...\n",
      "Features extracted and transformed\n",
      "Fitting regression...\n"
     ]
    }
   ],
   "source": [
    "from src.algonauts.evaluators import correlations as corr\n",
    "from src.algonauts.data_processors.nsd_dataset import NSDDataset\n",
    "\n",
    "for layer_name in layers:\n",
    "    print(f'Running for layer {layer_name}')\n",
    "    for subj in subjects:\n",
    "        print(f'Running for subject {subj}')\n",
    "\n",
    "        # Set data directories based on parameters\n",
    "        output_dir = f'{exp_output_dir}/{layer_name}'\n",
    "        dataset = NSDDataset(challenge_data_dir, output_dir, subj)\n",
    "\n",
    "        print('Loading datasets...')\n",
    "        train_ds, val_ds, test_ds = load_datasets(dataset, image_size, normalise_dataset)\n",
    "        print('Datasets loaded')\n",
    "\n",
    "        # Load model for the iteration\n",
    "        model = load_model()\n",
    "        sliced_model = tf.keras.Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
    "\n",
    "        # Train PCA\n",
    "        print('Training PCA...')\n",
    "        pca = train_pca(sliced_model, train_ds)\n",
    "        print('PCA over')\n",
    "\n",
    "        # Extract and transform features\n",
    "        print('Extracting and transforming features...')\n",
    "        train_features = extract_and_transform_features(train_ds, sliced_model, pca)\n",
    "        val_features = extract_and_transform_features(val_ds, sliced_model, pca)\n",
    "        test_features = extract_and_transform_features(test_ds, sliced_model, pca)\n",
    "        print('Features extracted and transformed')\n",
    "\n",
    "        # Delete model to free up memory\n",
    "        del model, pca\n",
    "\n",
    "        # Fit regression\n",
    "        print('Fitting regression...')\n",
    "        reg_lh = LinearRegression().fit(train_features, dataset.lh_fmri_train)\n",
    "        reg_rh = LinearRegression().fit(train_features, dataset.rh_fmri_train)\n",
    "        print('Regression fitted')\n",
    "        \n",
    "        # Use fitted linear regressions to predict the validation and test fMRI data\n",
    "        print('Predicting fMRI data...')\n",
    "        lh_fmri_val_pred = reg_lh.predict(val_features)\n",
    "        lh_fmri_test_pred = reg_lh.predict(test_features)\n",
    "        rh_fmri_val_pred = reg_rh.predict(val_features)\n",
    "        rh_fmri_test_pred = reg_rh.predict(test_features)\n",
    "        print('fMRI data predicted')\n",
    "        # Calculate correlations for each hemispher\n",
    "        print('Calculating correlations...')\n",
    "        lh_correlation = corr.calculate_correlation(lh_fmri_val_pred, dataset.lh_fmri_val)\n",
    "        rh_correlation = corr.calculate_correlation(rh_fmri_val_pred, dataset.rh_fmri_val)  \n",
    "        print('Correlations calculated')\n",
    "    \n",
    "        corr.plot_and_write_correlations(dataset, lh_correlation, rh_correlation, exp_output_dir, layer_name, subj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "365c028d1c840c93b0f470009ef966704a741407ca08d29f5bd0c0c6373757cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load COCO 2017 dataset with annotations\n",
    "dataset, info = tfds.load('coco/2017', with_info=True, data_dir='/notebooks/tensorflow_datasets')\n",
    "num_classes = 80 # number of classes in COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets_from_coco(dataset, transform_image, num_classes):\n",
    "    def encode_coco_categories(coco_categories, num_classes):\n",
    "        return tf.reduce_max(tf.one_hot(coco_categories, num_classes), axis=0)\n",
    "\n",
    "    def set_labels_as_zeros():\n",
    "        return tf.zeros(num_classes, tf.float32)\n",
    "\n",
    "    def preprocess(example, transform_image):\n",
    "        image = example['image']\n",
    "        image = transform_image(image)\n",
    "\n",
    "        coco_categories = example['objects']['label']\n",
    "        is_empty = tf.equal(tf.size(coco_categories), 0)\n",
    "        labels = tf.cond(is_empty, set_labels_as_zeros, lambda: encode_coco_categories(coco_categories, num_classes))\n",
    "        return image, labels\n",
    "\n",
    "    batch_size = 32\n",
    "\n",
    "    train_ds = dataset['train'].map(lambda x: preprocess(x, transform_image)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds = dataset['validation'].map(lambda x: preprocess(x, transform_image)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    # see a picture and its label\n",
    "    # for image, label in train_ds.take(1):\n",
    "    #     image_number = 3\n",
    "    #     plt.imshow(image[image_number])\n",
    "    #     # print(image[image_number][0])\n",
    "    #     plt.show()\n",
    "    #     print(label[image_number])\n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your own model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_image(image):\n",
    "    image = tf.image.resize(image, (227, 227))\n",
    "    image = tf.cast(image, tf.float32) / 127.5 - 1.0\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    variance = [0.229, 0.224, 0.225]\n",
    "    normalization_layer = Normalization(mean=mean, variance=variance)\n",
    "    image = normalization_layer(image)\n",
    "    return image\n",
    "\n",
    "train_ds, val_ds = create_datasets_from_coco(dataset, transform_image, num_classes)\n",
    "\n",
    "input_shape = (227, 227, 3)\n",
    "\n",
    "model = Sequential()\n",
    "# Convolutional layers\n",
    "model.add(Conv2D(96, (11, 11), strides=(4, 4), activation=\"relu\", input_shape=input_shape, padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((3, 3), strides=(2, 2)))\n",
    "model.add(Conv2D(256, (5, 5), activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((3, 3), strides=(2, 2)))\n",
    "model.add(Conv2D(384, (3, 3), activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(384, (3, 3), activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((3, 3), strides=(2, 2)))\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, activation=\"relu\", kernel_initializer=\"glorot_normal\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, activation=\"relu\", kernel_initializer=\"glorot_normal\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics='accuracy')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or load pretrained archtecture and change the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_image(image):\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    image = tf.keras.applications.vgg16.preprocess_input(image)\n",
    "    return image\n",
    "    \n",
    "train_ds, val_ds = create_datasets_from_coco(dataset, transform_image, num_classes)\n",
    "\n",
    "# Load the VGG16 model without the top layers\n",
    "base_model = tf.keras.applications.VGG16(weights=None, include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Add top layers for multi-label classification\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "# Create the final model\n",
    "model = tf.keras.Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Print the layer names\n",
    "print(*(layer.name for layer in model.layers), sep=' -> ')\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_ds, epochs=10, validation_data=val_ds)\n",
    "\n",
    "model.save('model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
